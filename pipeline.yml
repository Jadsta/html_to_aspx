name : Graph Data Pipeline

trigger: none

parameters:
  - name: awsenv
    displayName: 'AWS Env to Deploy'
    type: string
    values:
      - dev
      - qa
      - prod

  - name: action
    displayName: 'Action to Perform'
    type: string
    values:
      - ScheduleETL
      - ETLJobStatusReset
      - InitialiseETLConfig
      - UpdateETLConfig
      - DeployConfig

  - name: configScope
    displayName: 'Config Scope'
    type: string
    values:
      - all
      - specific
    default: specific

  - name: dataset
    displayName: 'Dataset Name (when configScope = specific)'
    type: string
    values:
      - dataset1
      - dataset2
      - dataset3

variables:
  - name: bucketName
    ${{ if eq(parameters.awsenv, 'dev') }}:
      value: 'graph-app-dev'
    ${{ if eq(parameters.awsenv, 'qa') }}:
      value: 'graph-app-qa'
    ${{ if eq(parameters.awsenv, 'prod') }}:
      value: 'graph-app-prod'
  - name: etlConfigTable
    value: 'your-dynamodb-table'
  - name: schemaMapTable
    value: 'your-schema-map-table'
  - name: awsCredentials
    value: '${{ parameters.awsenv }}-aws-service-connection'
  - name: awsRegion
    value: 'us-east-1'
  - name: protectedColumns
    value: '.etlFromDate, .lastRunTime, .jobStatus'

${{ if eq(parameters.action, 'InitialiseETLConfig') }}  
  - stage: InitialiseETLConfig
    displayName: 'Initialise ETL Job Config'
    jobs:
      - job: Deploy
        steps:
          - template: templates/common-functions.yml
            parameters:
              taskName: 'Initialise ETL Job Config'
              awsenv: ${{ parameters.awsenv }}
              configScope: ${{ parameters.configScope }}
              dataset: ${{ parameters.dataset }}
              bucketName: ${{ variables.bucketName }}
              etlConfigTable: ${{ variables.etlConfigTable }}
              schemaMapTable: ${{ variables.schemaMapTable }}
              awsCredentials: ${{ variables.awsCredentials }}
              awsRegion: ${{ variables.awsRegion }}
              protectedColumns: ''
              operationType: 'initialise'
              customScript: |
                echo "Initialising ETL Job Config - Scope: ${{ parameters.configScope }}"
                
                # Process datasets
                datasets=$(get_datasets)
                for dataset in $datasets; do
                  process_etl_config "$dataset"
                done
                
                echo "ETL Config Initialisation completed"

${{ if eq(parameters.action, 'UpdateETLConfig') }}  
  - stage: UpdateETLConfig
    displayName: 'Update ETL Job Config'
    jobs:
      - job: Deploy
        steps:
          - template: templates/common-functions.yml
            parameters:
              taskName: 'Update ETL Job Config'
              awsenv: ${{ parameters.awsenv }}
              configScope: ${{ parameters.configScope }}
              dataset: ${{ parameters.dataset }}
              bucketName: ${{ variables.bucketName }}
              etlConfigTable: ${{ variables.etlConfigTable }}
              schemaMapTable: ${{ variables.schemaMapTable }}
              awsCredentials: ${{ variables.awsCredentials }}
              awsRegion: ${{ variables.awsRegion }}
              protectedColumns: ${{ variables.protectedColumns }}
              operationType: 'update'
              customScript: |
                echo "Updating ETL Job Config - Scope: ${{ parameters.configScope }}"
                
                # Process datasets
                datasets=$(get_datasets)
                for dataset in $datasets; do
                  process_etl_config "$dataset"
                done
                
                echo "ETL Config Update completed"

${{ if or(eq(parameters.action, 'InitialiseETLConfig'), eq(parameters.action, 'UpdateETLConfig')) }}  
  - stage: DeploySchemaMap
    displayName: 'Deploy Schema Map'
    jobs:
      - job: Deploy
        steps:
          - template: templates/common-functions.yml
            parameters:
              taskName: 'Deploy Schema Map'
              awsenv: ${{ parameters.awsenv }}
              configScope: ${{ parameters.configScope }}
              dataset: ${{ parameters.dataset }}
              bucketName: ${{ variables.bucketName }}
              etlConfigTable: ${{ variables.etlConfigTable }}
              schemaMapTable: ${{ variables.schemaMapTable }}
              awsCredentials: ${{ variables.awsCredentials }}
              awsRegion: ${{ variables.awsRegion }}
              customScript: |
                echo "Deploying Schema Maps - Scope: ${{ parameters.configScope }}"
                
                # Function to deploy a single schema map
                deploy_schema_map() {
                  local dataset_name=$1
                  echo "Deploying schema map for dataset: $dataset_name"
                  
                  # Read schema map content
                  schema_file="dataflow/graph/schema-maps/graph-$dataset_name.json"
                  if [ ! -f "$schema_file" ]; then
                    echo "Warning: Schema map file $schema_file not found for dataset $dataset_name"
                    return 1
                  fi
                  
                  schema_content=$(cat "$schema_file" | jq -c .)
                  echo "Schema map loaded for dataset: $dataset_name"
                  
                  # Find jobKey using shared function
                  dataset_info=$(find_dataset_info "$dataset_name")
                  if [ $? -ne 0 ]; then
                    echo "Error: Could not find jobKey for dataset $dataset_name"
                    return 1
                  fi
                  
                  job_key=$(echo "$dataset_info" | cut -d'|' -f2)
                  echo "Found jobKey: $job_key for dataset: $dataset_name"
                  
                  # Escape schema content for JSON
                  escaped_schema=$(echo "$schema_content" | sed 's/\\/\\\\/g' | sed 's/"/\\"/g')
                  
                  # Build DynamoDB item
                  dynamodb_item="{\"collection\":{\"S\":\"$job_key\"},\"schema-map\":{\"S\":\"$escaped_schema\"}}"
                  
                  echo "DynamoDB item prepared for collection: $job_key"
                  
                  # Insert/Overwrite the item in DynamoDB
                  aws dynamodb put-item \
                    --table-name ${{ variables.schemaMapTable }} \
                    --item "$dynamodb_item"
                  
                  echo "Successfully deployed schema map for dataset: $dataset_name"
                }
                
                # Process datasets (use schema-maps directory for 'all' scope)
                if [ "${{ parameters.configScope }}" = "all" ]; then
                  echo "Processing all datasets..."
                  datasets=$(ls dataflow/graph/schema-maps/graph-*.json | sed 's/.*graph-\(.*\)\.json/\1/')
                  for dataset in $datasets; do
                    deploy_schema_map "$dataset"
                  done
                else
                  echo "Processing specific dataset: ${{ parameters.dataset }}"
                  deploy_schema_map "${{ parameters.dataset }}"
                fi
                
                echo "Schema Map Deployment completed"
